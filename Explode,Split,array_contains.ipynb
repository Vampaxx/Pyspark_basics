{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhrYwlxesVTiEqH8P19Xyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vampaxx/Pyspark_basics/blob/main/Explode%2CSplit%2Carray_contains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wnn8xKo5iOo",
        "outputId": "e8ba9c43-0121-4bd0-9c4f-47f1d0bfdef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=21d22a5da9649fc23a7ed967c55c1b1abe7532b4638ee87dd9c1fe275c04e7dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType,StructField,StructType,IntegerType,ArrayType\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "gvElLVeG5rHJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('Explode_split_array_contains').getOrCreate()"
      ],
      "metadata": {
        "id": "Hwqy8ATn5tiS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agenda\n",
        "\n",
        "- Explode()\n",
        "- Split()\n",
        "- Array()\n",
        "- Array_contains()"
      ],
      "metadata": {
        "id": "w24EOfsG56XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Explode()\n",
        "\n",
        "Use explode() function to create a new now for each element in the given array column."
      ],
      "metadata": {
        "id": "zZrmkyla6F8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,explode\n",
        "from pyspark.sql.functions import col,lit,when,expr"
      ],
      "metadata": {
        "id": "dynRiKNk6ZGI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    ('apple',[1,2],{'shop_A' : 'Mumbai','shop_B':'kolatha'}),\\\n",
        "    ('orange',[2,3],{'shop_A' : 'Kerala','shop_B':'banglroe'}),\\\n",
        "    ('grapes',[4,5],{'shop_A' : 'Chennai','shop_B':'Pune'})\n",
        "]"
      ],
      "metadata": {
        "id": "r0q1LV97Tz2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\\\n",
        "                     StructField('Fruits',StringType()),\\\n",
        "                     StructField('No_of_kgs',ArrayType(IntegerType())),\\\n",
        "                     StructField('Head_quaters',StringType())\n",
        "                     ])\n",
        "\n",
        "df = spark.createDataFrame(data,['Fruits','No_of_kgs','Head_quaters'])"
      ],
      "metadata": {
        "id": "8mLUXvu7UBqS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZHR2h0uUlQC",
        "outputId": "36d19046-e268-47df-a017-bd19eff7cb45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Fruits: string (nullable = true)\n",
            " |-- No_of_kgs: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- Head_quaters: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OW7vL7mVSnI",
        "outputId": "ded77e2a-5a64-4167-a6e0-8047e818fd85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+--------------------------------------+\n",
            "|Fruits|No_of_kgs|Head_quaters                          |\n",
            "+------+---------+--------------------------------------+\n",
            "|apple |[1, 2]   |{shop_B -> kolatha, shop_A -> Mumbai} |\n",
            "|orange|[2, 3]   |{shop_B -> banglroe, shop_A -> Kerala}|\n",
            "|grapes|[4, 5]   |{shop_B -> Pune, shop_A -> Chennai}   |\n",
            "+------+---------+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('apple',[1,2],['Banglore','Austin']),\\\n",
        "        ('orange',[4,5],['Chennai','New_york']),\\\n",
        "        ('grapes',[7,8],['Kochi','DC'])]\n",
        "\n",
        "schema = StructType([\\\n",
        "                     StructField('fruits',StringType()),\\\n",
        "                     StructField('No_of_kgs',ArrayType(IntegerType())),\\\n",
        "                     StructField('Metro_city',ArrayType(StringType()))\n",
        "                     ]) #Interger present inside the array type\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df1 = df.withColumn('explode_col',explode(col('No_of_kgs'))).select('fruits','explode_col')\n",
        "print('Before explode')\n",
        "df.show()\n",
        "print('after explode')\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U18EYb6O6hZS",
        "outputId": "87218fb6-ed9f-426b-d410-3d327b698163"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before explode\n",
            "+------+---------+-------------------+\n",
            "|fruits|No_of_kgs|         Metro_city|\n",
            "+------+---------+-------------------+\n",
            "| apple|   [1, 2]| [Banglore, Austin]|\n",
            "|orange|   [4, 5]|[Chennai, New_york]|\n",
            "|grapes|   [7, 8]|        [Kochi, DC]|\n",
            "+------+---------+-------------------+\n",
            "\n",
            "after explode\n",
            "+------+-----------+\n",
            "|fruits|explode_col|\n",
            "+------+-----------+\n",
            "| apple|          1|\n",
            "| apple|          2|\n",
            "|orange|          4|\n",
            "|orange|          5|\n",
            "|grapes|          7|\n",
            "|grapes|          8|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Multiple column explode()\n",
        "\n",
        "\n",
        "df.withColumn('total_kgs',explode('No_of_kgs'))\\\n",
        "  .withColumn('Metro_',explode('Metro_city'))\\\n",
        "  .withColumn('Metro_',explode('Metro_city'))\\\n",
        "  .select('fruits','total_kgs','Metro_').show()"
      ],
      "metadata": {
        "id": "6Xc5E7H0O9pC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b531c223-c049-43fe-cde5-de9a305c91de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+--------+\n",
            "|fruits|total_kgs|  Metro_|\n",
            "+------+---------+--------+\n",
            "| apple|        1|Banglore|\n",
            "| apple|        1|  Austin|\n",
            "| apple|        1|Banglore|\n",
            "| apple|        1|  Austin|\n",
            "| apple|        2|Banglore|\n",
            "| apple|        2|  Austin|\n",
            "| apple|        2|Banglore|\n",
            "| apple|        2|  Austin|\n",
            "|orange|        4| Chennai|\n",
            "|orange|        4|New_york|\n",
            "|orange|        4| Chennai|\n",
            "|orange|        4|New_york|\n",
            "|orange|        5| Chennai|\n",
            "|orange|        5|New_york|\n",
            "|orange|        5| Chennai|\n",
            "|orange|        5|New_york|\n",
            "|grapes|        7|   Kochi|\n",
            "|grapes|        7|      DC|\n",
            "|grapes|        7|   Kochi|\n",
            "|grapes|        7|      DC|\n",
            "+------+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uObYZY3U3UYh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.withColumn('shop_1',col('No_of_kgs').getItem(0))\\\n",
        "        .withColumn('shop_2',col('No_of_kgs').getItem(1))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs5yldsg8QnC",
        "outputId": "1b2e3f2a-952a-4614-f4e7-316c1e9b0ef0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+------+------+\n",
            "|fruits|No_of_kgs|shop_1|shop_2|\n",
            "+------+---------+------+------+\n",
            "| apple|   [1, 2]|     1|     2|\n",
            "|orange|   [4, 5]|     4|     5|\n",
            "|grapes|   [7, 8]|     7|     8|\n",
            "+------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'maheer',['dotnet','azure']),\n",
        "        (2,'wafa',['java','aws']),\n",
        "        (3,'Micheal',['aws','pyhton'])]\n",
        "\n",
        "schema = StructType([\\\n",
        "                     StructField('Emp_id',IntegerType()),\\\n",
        "                     StructField('Emp_name',StringType()),\\\n",
        "                     StructField('skills',ArrayType(StringType()))\n",
        "                     ])\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "spark.createDataFrame(data,schema).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT5cn-R1NhxV",
        "outputId": "6dd177dc-38d7-4cb2-a3a9-6b89d79e38c7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+\n",
            "|Emp_id|Emp_name|         skills|\n",
            "+------+--------+---------------+\n",
            "|     1|  maheer|[dotnet, azure]|\n",
            "|     2|    wafa|    [java, aws]|\n",
            "|     3| Micheal|  [aws, pyhton]|\n",
            "+------+--------+---------------+\n",
            "\n",
            "root\n",
            " |-- Emp_id: integer (nullable = true)\n",
            " |-- Emp_name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split()\n",
        "\n",
        "Split() function returns an array type after splitting the string column by delimiter."
      ],
      "metadata": {
        "id": "HTExvl7_QCYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split"
      ],
      "metadata": {
        "id": "G-js9Xh9e8XP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\\\n",
        "        (1,'Micheal','.net,azure,sql'),\n",
        "        (2,'Daniel','java,aws,sql')]\n",
        "schema = ['Emp_id','Emp_name','skills']\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG-gbV-wQRFS",
        "outputId": "87c8fa43-aba3-40a7-cfdb-1837c04a5414"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------------+\n",
            "|Emp_id|Emp_name|        skills|\n",
            "+------+--------+--------------+\n",
            "|     1| Micheal|.net,azure,sql|\n",
            "|     2|  Daniel|  java,aws,sql|\n",
            "+------+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('skill_set',split(col('skills'),',')).select('Emp_id','Emp_name','skill_set').show()"
      ],
      "metadata": {
        "id": "MRnUSANjQ7pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23444a47-9a74-4cd4-8b14-18079152ebf8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------------------+\n",
            "|Emp_id|Emp_name|         skill_set|\n",
            "+------+--------+------------------+\n",
            "|     1| Micheal|[.net, azure, sql]|\n",
            "|     2|  Daniel|  [java, aws, sql]|\n",
            "+------+--------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6R0LJGU3PeMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Array()\n",
        "\n",
        "\n",
        "Use array() function to create a new array column by merging the data from multiple columns"
      ],
      "metadata": {
        "id": "iw_9v2quf-a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array"
      ],
      "metadata": {
        "id": "DEReUsMNPOdj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\\\n",
        "        (1,'Micheal','azure','sql'),\n",
        "        (2,'Daniel','aws','python')]\n",
        "schema = ['Emp_id','Emp_name','primary_skills','secondary_skills']\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "6GbjMvLFPPRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a7eff1-0493-48da-b169-6bce9fc6ea8e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------------+----------------+\n",
            "|Emp_id|Emp_name|primary_skills|secondary_skills|\n",
            "+------+--------+--------------+----------------+\n",
            "|     1| Micheal|         azure|             sql|\n",
            "|     2|  Daniel|           aws|          python|\n",
            "+------+--------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('skills',array(col('primary_skills'),col('secondary_skills'))).select('Emp_id','Emp_name','skills').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBQBVYNzgkXu",
        "outputId": "66e0d467-ec3c-427e-ef2c-662d0859fc93"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------------+\n",
            "|Emp_id|Emp_name|       skills|\n",
            "+------+--------+-------------+\n",
            "|     1| Micheal| [azure, sql]|\n",
            "|     2|  Daniel|[aws, python]|\n",
            "+------+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pb59gRS-Og29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Array_contain()\n",
        "\n",
        "This function used to check if array column contain a value. Return Null if the array is null , True if the array contain the value and False otherwise"
      ],
      "metadata": {
        "id": "7Yie7GcxrPYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains"
      ],
      "metadata": {
        "id": "wCvQ6eugNbfG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "Jx3cSNX6M8du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64aff546-e4cb-40b6-da85-17c43e13522f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+\n",
            "|Emp_id|Emp_name|         skills|\n",
            "+------+--------+---------------+\n",
            "|     1|  maheer|[dotnet, azure]|\n",
            "|     2|    wafa|    [java, aws]|\n",
            "|     3| Micheal|  [aws, pyhton]|\n",
            "+------+--------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('has_AWS_skills',array_contains(col('skills'),'aws')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hksZEmIjsM_Z",
        "outputId": "d63cb899-b9a0-4677-ace4-c0187e82f317"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+--------------+\n",
            "|Emp_id|Emp_name|         skills|has_AWS_skills|\n",
            "+------+--------+---------------+--------------+\n",
            "|     1|  maheer|[dotnet, azure]|         false|\n",
            "|     2|    wafa|    [java, aws]|          true|\n",
            "|     3| Micheal|  [aws, pyhton]|          true|\n",
            "+------+--------+---------------+--------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}